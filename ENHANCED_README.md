# ğŸš€ SmartScrape Full-Stack - Enhanced Edition

A comprehensive, enterprise-grade web scraping platform powered by **Crawl4AI**, **Cheerio**, and modern web technologies. This enhanced edition transforms basic web scraping into an intelligent, scalable, and production-ready solution.

## âœ¨ What's New in Enhanced Edition

### ğŸ”„ Core Engine Improvements
- **Switched from HTMLRewriter to Cheerio** for superior HTML parsing capabilities
- **Integrated Crawl4AI Python service** for advanced, AI-powered crawling
- **Enhanced pagination support** with auto-detection and CSS selector strategies
- **Intelligent content extraction** with multiple extraction strategies

### ğŸ›¡ï¸ Advanced Proxy System  
- **Comprehensive proxy health monitoring** with performance metrics
- **Smart proxy rotation** with reliability scoring
- **Real-time proxy testing** and automatic failover
- **Detailed analytics** on proxy performance and uptime

### ğŸ¨ Modern UI & UX
- **Real-time progress tracking** with live updates
- **Monaco Editor integration** for extraction schema editing
- **Responsive design** with Tailwind CSS
- **Interactive dashboards** with Chart.js visualizations
- **Enhanced notifications** and status indicators

### ğŸ§ª Testing & Quality Assurance
- **Comprehensive test suite** covering all functionality
- **Performance benchmarking** and monitoring
- **Integration testing** for all components
- **Automated health checks** and system monitoring

## ğŸŒŸ Key Features

### ğŸ•·ï¸ Advanced Web Crawling
- **Multiple crawling strategies**: Basic, Smart, AI-powered extraction
- **Deep crawling capabilities** with configurable depth limits
- **Pagination handling** with auto-detection and custom selectors
- **JavaScript rendering** support through Crawl4AI integration
- **Session management** with resumable crawling operations

### ğŸ¯ Intelligent Data Extraction
- **CSS Selector extraction** for precise data targeting
- **XPath support** for complex element selection
- **AI-powered extraction** using LLM-based strategies
- **Custom extraction schemas** with JSON configuration
- **Content cleaning** and normalization

### ğŸ“Š Analytics & Monitoring
- **Real-time performance metrics** and dashboards
- **Crawl session analytics** with detailed reporting
- **Proxy performance tracking** and optimization
- **System health monitoring** with automated alerts
- **Historical data analysis** and trending

### ğŸ”§ Enterprise Features
- **Scalable architecture** supporting high-volume operations
- **Rate limiting** and request throttling
- **Error handling** and automatic retry mechanisms
- **Export capabilities** (JSON, CSV, Markdown)
- **API-first design** with comprehensive endpoints

## ğŸ—ï¸ Enhanced Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Frontend (React-like)       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”‚ Dashboard   â”‚ Real-time Updates       â”‚
â”‚  â”‚ Analytics   â”‚ Monaco Editor           â”‚
â”‚  â”‚ Sessions    â”‚ Chart Visualizations    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           Hono API Layer                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”‚ Crawl API   â”‚ Enhanced Routing        â”‚
â”‚  â”‚ Proxy API   â”‚ Error Handling          â”‚
â”‚  â”‚ Test API    â”‚ Real-time Events        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Enhanced Crawler Engine          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”‚ Cheerio     â”‚ Smart Extraction        â”‚
â”‚  â”‚ Processing  â”‚ Pagination Detection    â”‚
â”‚  â”‚ Concurrency â”‚ Content Cleaning        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Crawl4AI Python Service         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”‚ Advanced    â”‚ LLM Integration         â”‚
â”‚  â”‚ Crawling    â”‚ AI Extraction           â”‚
â”‚  â”‚ JavaScript  â”‚ Screenshot Capture      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      Enhanced Proxy Management          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”‚ Health      â”‚ Performance Tracking    â”‚
â”‚  â”‚ Monitoring  â”‚ Auto-cleanup            â”‚
â”‚  â”‚ Load Bal.   â”‚ Reliability Scoring     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Cloudflare D1 Database           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”‚ Sessions    â”‚ Enhanced Schema         â”‚
â”‚  â”‚ URLs        â”‚ Performance Metrics     â”‚
â”‚  â”‚ Proxies     â”‚ Analytics Tables        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- **Node.js 18+**
- **Python 3.9+** (for Crawl4AI service)
- **npm or yarn**
- **PM2** (for process management)

### Installation

1. **Clone the enhanced repository**
   ```bash
   git clone https://github.com/jonquixote/SmartScrape-Full-Stack.git
   cd SmartScrape-Full-Stack
   git checkout genspark_ai_developer
   ```

2. **Install Node.js dependencies**
   ```bash
   npm install
   ```

3. **Setup Python Crawl4AI service** (Optional but recommended)
   ```bash
   pip install -r requirements.txt
   ```

4. **Initialize database**
   ```bash
   npm run db:migrate:local
   ```

5. **Build the project**
   ```bash
   npm run build
   ```

6. **Start the enhanced services**
   ```bash
   # Start main application with PM2
   pm2 start ecosystem.config.cjs
   
   # Start Crawl4AI Python service (optional)
   python crawl4ai-service.py
   ```

7. **Access the application**
   - **Main App**: http://localhost:3000
   - **API Health**: http://localhost:3000/api/health
   - **Crawl4AI Service**: http://localhost:8000 (if running)

## ğŸ“‹ Enhanced API Endpoints

### Core Crawling
- `POST /api/crawl/sessions` - Create crawl session with advanced options
- `POST /api/crawl/sessions/:id/start` - Start enhanced crawling
- `GET /api/crawl/sessions/:id/progress` - Real-time progress tracking
- `GET /api/crawl/sessions/:id/analytics` - Session analytics

### Advanced Features
- `POST /api/crawl/test-extraction` - Test extraction schemas
- `POST /api/crawl/suggest-selectors` - AI-powered selector suggestions
- `GET /api/crawl/sessions/:id/tree` - Crawl tree visualization

### Enhanced Proxy Management
- `GET /api/proxies` - List proxies with performance metrics
- `POST /api/proxies/test` - Advanced proxy testing
- `GET /api/proxies/health` - Proxy health monitoring
- `GET /api/proxies/stats` - Comprehensive proxy analytics
- `DELETE /api/proxies/cleanup` - Automatic proxy cleanup

### System Monitoring
- `GET /api/health` - System health check
- `POST /api/test/crawler` - Run comprehensive tests
- `GET /api/system/metrics` - Performance metrics

## ğŸ¯ Enhanced Usage Examples

### Advanced Crawl Session
```javascript
const session = await fetch('/api/crawl/sessions', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    title: 'E-commerce Product Crawl',
    description: 'Extract product data with pricing',
    start_method: 'manual',
    urls: ['https://example-store.com/products'],
    
    // Enhanced crawling options
    crawl_strategy: 'smart',
    enable_deep_crawl: true,
    enable_pagination: true,
    pagination_strategy: 'auto',
    max_depth: 3,
    max_concurrent: 5,
    
    // Advanced extraction
    ai_extraction_schema: {
      fields: [
        { name: 'title', selector: 'h1.product-title', attribute: 'text' },
        { name: 'price', selector: '.price', attribute: 'text' },
        { name: 'images', selector: 'img.product-image', attribute: 'src', multiple: true }
      ]
    },
    
    // Content processing
    generate_markdown: true,
    extract_metadata: true,
    smart_cleaning: true,
    remove_ads: true
  })
});
```

### Real-time Progress Monitoring
```javascript
// Monitor crawl progress with WebSocket-like updates
const monitorProgress = async (sessionId) => {
  const eventSource = new EventSource(`/api/crawl/sessions/${sessionId}/progress`);
  
  eventSource.onmessage = (event) => {
    const progress = JSON.parse(event.data);
    updateProgressBar(progress.progress_percentage);
    updateStatus(progress.status);
    updateMetrics(progress.urls_completed, progress.urls_failed);
  };
};
```

### Advanced Proxy Testing
```javascript
const testResults = await fetch('/api/proxies/test', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    concurrent: 10,
    test_url: 'https://httpbin.org/ip'
  })
});

const { summary, results } = await testResults.json();
console.log(`Success rate: ${summary.success_rate}%`);
console.log(`Average response time: ${summary.average_response_time}ms`);
```

## ğŸ§ª Testing Suite

The enhanced edition includes comprehensive testing:

```bash
# Run all tests
npm run test:all

# Run specific test suites
npm run test:crawler    # Crawler functionality
npm run test:proxy      # Proxy system
npm run test:extraction # Data extraction
npm run test:integration # End-to-end tests

# Performance benchmarking
npm run test:performance
```

### Test Coverage
- âœ… **Crawler Engine Testing** - All crawling scenarios
- âœ… **Proxy System Testing** - Health monitoring and failover
- âœ… **Extraction Testing** - Schema validation and data accuracy
- âœ… **API Integration Testing** - End-to-end workflows
- âœ… **Performance Testing** - Load and stress testing
- âœ… **Error Handling Testing** - Edge cases and failure modes

## ğŸ”§ Configuration

### Environment Variables
```bash
# Optional Crawl4AI service
CRAWL4AI_ENDPOINT=http://localhost:8000
CRAWL4AI_API_KEY=your-api-key

# Advanced features
GROQ_API_KEY=your-groq-key
OPENAI_API_KEY=your-openai-key

# Monitoring
ENABLE_ANALYTICS=true
LOG_LEVEL=info
```

### Advanced Configuration
```json
{
  "crawling": {
    "default_concurrency": 5,
    "max_concurrency": 20,
    "default_delay": 1000,
    "max_depth": 10,
    "timeout": 30000
  },
  "proxy": {
    "health_check_interval": 300000,
    "max_consecutive_failures": 10,
    "min_success_rate": 70
  },
  "extraction": {
    "enable_ai_fallback": true,
    "max_content_length": 1000000,
    "enable_smart_cleaning": true
  }
}
```

## ğŸ“Š Performance & Monitoring

### Dashboard Metrics
- **Real-time crawl statistics** with live updates
- **Proxy performance monitoring** with health indicators
- **System resource usage** and optimization suggestions
- **Error tracking** and debugging information
- **Historical performance** trends and analytics

### Performance Optimizations
- **Intelligent concurrency control** based on target response times
- **Adaptive rate limiting** to prevent blocking
- **Smart proxy rotation** based on performance metrics
- **Content caching** and deduplication
- **Memory optimization** for large-scale operations

## ğŸ”’ Security & Compliance

### Security Features
- **Input validation** and sanitization
- **SQL injection prevention** with parameterized queries
- **Rate limiting** and request throttling
- **Secure proxy handling** with credential management
- **Error message sanitization** to prevent information leakage

### Compliance Considerations
- **Robots.txt respect** (configurable)
- **Rate limiting** to be respectful to target sites
- **User-Agent rotation** for ethical crawling
- **Data retention policies** and cleanup procedures

## ğŸ“ˆ Scalability & Production

### Production Deployment
```bash
# Build for production
npm run build

# Deploy to Cloudflare Pages
npm run deploy:prod

# Setup production database
npm run db:migrate:prod
```

### Scaling Considerations
- **Horizontal scaling** with multiple worker instances
- **Database optimization** with proper indexing
- **Caching strategies** for frequently accessed data
- **Load balancing** for high-availability setups
- **Monitoring and alerting** for production environments

## ğŸ¤ Contributing

We welcome contributions to enhance SmartScrape further:

1. **Fork the repository**
2. **Create a feature branch** (`git checkout -b feature/amazing-feature`)
3. **Make your changes** with proper testing
4. **Commit your changes** (`git commit -m 'Add amazing feature'`)
5. **Push to the branch** (`git push origin feature/amazing-feature`)
6. **Open a Pull Request**

## ğŸ“ Changelog

### v2.0.0 - Enhanced Edition
- ğŸ”„ Complete crawler engine rewrite with Cheerio
- ğŸ Crawl4AI Python service integration
- ğŸ“„ Advanced pagination detection and handling
- ğŸ›¡ï¸ Comprehensive proxy performance monitoring
- ğŸ¨ Modern UI with real-time updates
- ğŸ§ª Complete test suite implementation
- ğŸ“Š Advanced analytics and reporting
- ğŸ”§ Enhanced error handling and resilience

## ğŸ“ Support

- **Documentation**: [Enhanced API Docs](./docs/api.md)
- **Issues**: [GitHub Issues](https://github.com/jonquixote/SmartScrape-Full-Stack/issues)
- **Discussions**: [GitHub Discussions](https://github.com/jonquixote/SmartScrape-Full-Stack/discussions)

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**ğŸŒŸ SmartScrape Enhanced Edition - Where Intelligence Meets Web Scraping**

Built with â¤ï¸ using **Crawl4AI**, **Hono**, **Cheerio**, **Cloudflare Pages**, and modern web technologies.